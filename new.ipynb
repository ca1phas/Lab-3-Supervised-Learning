{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a1c206d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. days: 185\n",
      "Date range: 2021-07-01 00:00:00 to 2022-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# CONSTANTS\n",
    "TARGET = \"cases_new_increase_tmr\"\n",
    "START_DATE = pd.to_datetime(\"2021-07-01\")\n",
    "END_DATE = START_DATE + pd.DateOffset(months=6)\n",
    "NO_DAYS = (END_DATE - START_DATE).days + 1\n",
    "dates = pd.date_range(start=START_DATE, end=END_DATE, freq=\"D\")\n",
    "print(f\"No. days: {NO_DAYS}\")\n",
    "print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "\n",
    "# Base GitHub URL\n",
    "BASE_URL = \"https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/\"\n",
    "\n",
    "# Relevant CSVs\n",
    "files = {\n",
    "    \"cases_malaysia\": BASE_URL + \"epidemic/cases_malaysia.csv\",\n",
    "    \"tests_malaysia\": BASE_URL + \"epidemic/tests_malaysia.csv\",\n",
    "    \"checkin_malaysia\": BASE_URL + \"mysejahtera/checkin_malaysia.csv\",\n",
    "    # \"deaths_malaysia\": BASE_URL + \"epidemic/deaths_malaysia.csv\",\n",
    "    # \"hospital\": BASE_URL + \"epidemic/hospital.csv\",\n",
    "    # \"icu\": BASE_URL + \"epidemic/icu.csv\",\n",
    "    # \"vax_malaysia\": BASE_URL + \"vaccination/vax_malaysia.csv\",\n",
    "    # \"trace_malaysia\": BASE_URL + \"mysejahtera/trace_malaysia.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b1130495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases_malaysia: (185, 24)\n",
      "tests_malaysia: (185, 3)\n",
      "checkin_malaysia: (185, 4)\n"
     ]
    }
   ],
   "source": [
    "# Extract and laod all CSVs into DataFrames\n",
    "dfs = {name: pd.read_csv(url) for name, url in files.items()}\n",
    "\n",
    "# Clean data\n",
    "for k, df in dfs.items():\n",
    "    # Set date column as data\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df = df[(df[\"date\"] >= START_DATE) & (df[\"date\"] <= END_DATE)]\n",
    "\n",
    "    # Drop columns where ALL values are null\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Clean data\n",
    "    if k == \"trace_malaysia\":\n",
    "        # Handle duplicates\n",
    "        df = df.groupby(\"date\", as_index=False).mean()\n",
    "\n",
    "        # Interpolate data for missing dates\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df.set_index(\"date\", inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        df = df.reindex(dates)\n",
    "        cols = [\"casual_contacts\", \"hide_large\", \"hide_small\"]\n",
    "        df[cols] = df[cols].interpolate(method=\"linear\")\n",
    "        df = df.reset_index().rename(columns={\"index\": \"date\"})\n",
    "\n",
    "        # Fix columns type\n",
    "        df[cols] = df[cols].round().astype(int)\n",
    "\n",
    "    # Remove columns with one value only\n",
    "    df = df.loc[:, df.nunique(dropna=True) > 1]\n",
    "\n",
    "    # Save back into dictionary\n",
    "    dfs[k] = df\n",
    "\n",
    "    # Check shape\n",
    "    output_filename = f\"{k}.csv\"\n",
    "    print(f\"{k}: {df.shape}\")\n",
    "\n",
    "# Merge data\n",
    "data = pd.DataFrame({'date': dates})\n",
    "for k, df in dfs.items():\n",
    "    if k == \"population\":\n",
    "        continue\n",
    "    if df.shape[0] != NO_DAYS:\n",
    "        df = df.drop(columns=[\"state\"])\n",
    "        df = df.groupby(\"date\").sum()\n",
    "    data = pd.merge(data, df, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "40e98f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (178, 21)\n",
      "y shape: (178,)\n",
      "y distribution:  cases_new_increase_tmr\n",
      "0    93\n",
      "1    85\n",
      "Name: count, dtype: int64\n",
      "Selected features: ['cases_new', 'cases_active', 'cases_cluster', 'tests_total', 'mobility_density', 'cases_new_shift1', 'cases_new_7d_avg', 'cases_new_pct_change', 'cases_active_shift1', 'cases_active_7d_avg', 'cases_active_pct_change', 'cases_cluster_shift1', 'cases_cluster_7d_avg', 'cases_cluster_pct_change', 'tests_total_shift1', 'tests_total_7d_avg', 'tests_total_pct_change', 'mobility_density_shift1', 'mobility_density_7d_avg', 'mobility_density_pct_change', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "# Include target cases_new_increase_tmr\n",
    "data[\"cases_new_increase_tmr\"] = (\n",
    "    data[\"cases_new\"].shift(-1) > data[\"cases_new\"]\n",
    ").astype(int)\n",
    "\n",
    "# Drop last row (no target)\n",
    "data = data[:-1]\n",
    "\n",
    "# Features selection\n",
    "base_features = [\n",
    "    \"cases_new\",\n",
    "    \"cases_active\",\n",
    "    \"cases_cluster\",\n",
    "    \"tests_total\",\n",
    "    \"mobility_density\",\n",
    "]\n",
    "\n",
    "feature_cols = base_features.copy()\n",
    "\n",
    "# Combine features\n",
    "data[\"tests_total\"] = data[\"rtk-ag\"] + data[\"pcr\"]  # combine tests\n",
    "data[\"mobility_density\"] = (\n",
    "    data[\"checkins\"] / data[\"unique_loc\"]\n",
    ")  # density of people per location\n",
    "data[\"mobility_density\"] = (\n",
    "    data[\"mobility_density\"].replace([float(\"inf\"), -float(\"inf\")], 0).fillna(0)\n",
    ")\n",
    "\n",
    "lag_days = [1]\n",
    "for col in base_features:\n",
    "    # Lag features\n",
    "    for day in lag_days:\n",
    "        feat_name =f\"{col}_shift{day}\" \n",
    "        data[feat_name] = data[col].shift(day)  # previous day\n",
    "        feature_cols.append(feat_name)\n",
    "\n",
    "    # Rolling averages (7-day)\n",
    "    feat_name = f\"{col}_7d_avg\"\n",
    "    data[feat_name] = data[col].rolling(window=7).mean()  # 7-day avg\n",
    "    feature_cols.append(feat_name)\n",
    "\n",
    "    # Percent change\n",
    "    feat_name = f\"{col}_pct_change\"\n",
    "    data[feat_name] = data[col].pct_change()  # daily pct change\n",
    "    feature_cols.append(feat_name)\n",
    "\n",
    "# Day of week\n",
    "data[\"day_of_week\"] = data[\"date\"].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "feature_cols.append(\"day_of_week\")\n",
    "\n",
    "# Drop rows with NaN caused by lag/rolling\n",
    "data = data.dropna().reset_index(drop=True)  # clean data\n",
    "\n",
    "# Target and features\n",
    "y = data[\"cases_new_increase_tmr\"]  # target\n",
    "X = data[feature_cols]  # features\n",
    "\n",
    "# Output shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"y distribution: \", y.value_counts())\n",
    "print(\"Selected features:\", feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c54fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model_split(\n",
    "    model, param_grid, X, y, scale_data=False, n_splits=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model using TimeSeriesSplit + GridSearchCV.\n",
    "    Splits data into train/validation folds sequentially, and uses the\n",
    "    last fold as the final test set.\n",
    "    Returns results DataFrame with metrics, best params, timings, and classification report.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = type(model).__name__\n",
    "    print(f\"\\n=== Training {model_name} with TimeSeriesSplit ===\")\n",
    "\n",
    "    # Initialize time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    # Use the last split as train/validation/test separation\n",
    "    splits = list(tscv.split(X))\n",
    "    train_idx, test_idx = splits[-1]  # last split\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Scaling if needed\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train = X_train.values\n",
    "        X_test = X_test.values\n",
    "\n",
    "    # GridSearchCV (with TimeSeriesSplit on training set only)\n",
    "    start_train = time.time()\n",
    "    grid = GridSearchCV(model, param_grid, cv=TimeSeriesSplit(n_splits=n_splits-1), scoring=\"f1\", n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    print(f\"[{model_name}] Best params: {grid.best_params_}\")\n",
    "    print(f\"[{model_name}] Training time: {train_time:.2f} seconds\")\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Inference\n",
    "    start_infer = time.time()\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    infer_time = time.time() - start_infer\n",
    "    print(f\"[{model_name}] Inference time: {infer_time:.4f} seconds\")\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    print(\n",
    "        f\"[{model_name}] Results - Accuracy: {acc:.4f}, \"\n",
    "        f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\"\n",
    "    )\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save to results DataFrame\n",
    "    df_results = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1,\n",
    "                \"best_params\": grid.best_params_,\n",
    "                \"train_time_sec\": train_time,\n",
    "                \"infer_time_sec\": infer_time,\n",
    "                \"confusion_matrix\": cm,\n",
    "                \"classification_report\": report,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_avg = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"average_accuracy\": acc,\n",
    "                \"average_precision\": precision,\n",
    "                \"average_recall\": recall,\n",
    "                \"average_f1\": f1,\n",
    "                \"avg_train_time_sec\": train_time,\n",
    "                \"avg_infer_time_sec\": infer_time,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== Finished {model_name} ===\")\n",
    "    print(\n",
    "        f\"Accuracy: {acc:.4f}, Precision: {precision:.4f}, \"\n",
    "        f\"Recall: {recall:.4f}, F1: {f1:.4f}, \"\n",
    "        f\"Train Time: {train_time:.2f}s, Infer Time: {infer_time:.4f}s\"\n",
    "    )\n",
    "\n",
    "    return df_results, df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aa1a4474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running LogisticRegression ---\n",
      "\n",
      "=== Training LogisticRegression with TimeSeriesSplit ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression] Best params: {'C': np.float64(0.31622776601683794), 'penalty': 'l2', 'solver': 'saga'}\n",
      "[LogisticRegression] Training time: 0.39 seconds\n",
      "[LogisticRegression] Inference time: 0.0005 seconds\n",
      "[LogisticRegression] Results - Accuracy: 0.8966, Precision: 0.8571, Recall: 0.9231, F1: 0.8889\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       0.86      0.92      0.89        13\n",
      "\n",
      "    accuracy                           0.90        29\n",
      "   macro avg       0.90      0.90      0.90        29\n",
      "weighted avg       0.90      0.90      0.90        29\n",
      "\n",
      "\n",
      "=== Finished LogisticRegression ===\n",
      "Accuracy: 0.8966, Precision: 0.8571, Recall: 0.9231, F1: 0.8889, Train Time: 0.39s, Infer Time: 0.0005s\n",
      "\n",
      "--- Running RandomForest ---\n",
      "\n",
      "=== Training RandomForestClassifier with TimeSeriesSplit ===\n",
      "[RandomForestClassifier] Best params: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "[RandomForestClassifier] Training time: 6.30 seconds\n",
      "[RandomForestClassifier] Inference time: 0.0456 seconds\n",
      "[RandomForestClassifier] Results - Accuracy: 0.8966, Precision: 0.8571, Recall: 0.9231, F1: 0.8889\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       0.86      0.92      0.89        13\n",
      "\n",
      "    accuracy                           0.90        29\n",
      "   macro avg       0.90      0.90      0.90        29\n",
      "weighted avg       0.90      0.90      0.90        29\n",
      "\n",
      "\n",
      "=== Finished RandomForestClassifier ===\n",
      "Accuracy: 0.8966, Precision: 0.8571, Recall: 0.9231, F1: 0.8889, Train Time: 6.30s, Infer Time: 0.0456s\n",
      "\n",
      "--- Running XGB ---\n",
      "\n",
      "=== Training XGBClassifier with TimeSeriesSplit ===\n",
      "[XGBClassifier] Best params: {'colsample_bytree': 0.8, 'gamma': 1, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 1.0}\n",
      "[XGBClassifier] Training time: 7.85 seconds\n",
      "[XGBClassifier] Inference time: 0.0020 seconds\n",
      "[XGBClassifier] Results - Accuracy: 0.8966, Precision: 0.8571, Recall: 0.9231, F1: 0.8889\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       0.86      0.92      0.89        13\n",
      "\n",
      "    accuracy                           0.90        29\n",
      "   macro avg       0.90      0.90      0.90        29\n",
      "weighted avg       0.90      0.90      0.90        29\n",
      "\n",
      "\n",
      "=== Finished XGBClassifier ===\n",
      "Accuracy: 0.8966, Precision: 0.8571, Recall: 0.9231, F1: 0.8889, Train Time: 7.85s, Infer Time: 0.0020s\n",
      "\n",
      "Results per model:\n",
      "                model  accuracy  f1_score  \\\n",
      "0  LogisticRegression  0.896552  0.888889   \n",
      "1        RandomForest  0.896552  0.888889   \n",
      "2                 XGB  0.896552  0.888889   \n",
      "\n",
      "                                         best_params    confusion_matrix  \n",
      "0  {'C': 0.31622776601683794, 'penalty': 'l2', 's...  [[14, 2], [1, 12]]  \n",
      "1  {'max_depth': None, 'max_features': 'sqrt', 'm...  [[14, 2], [1, 12]]  \n",
      "2  {'colsample_bytree': 0.8, 'gamma': 1, 'learnin...  [[14, 2], [1, 12]]  \n",
      "\n",
      "Average metrics per model:\n",
      "                model  average_accuracy  average_precision  average_recall  \\\n",
      "0  LogisticRegression          0.896552           0.857143        0.923077   \n",
      "1        RandomForest          0.896552           0.857143        0.923077   \n",
      "2                 XGB          0.896552           0.857143        0.923077   \n",
      "\n",
      "   average_f1  avg_train_time_sec  avg_infer_time_sec  \n",
      "0    0.888889            0.391099            0.000461  \n",
      "1    0.888889            6.303957            0.045610  \n",
      "2    0.888889            7.853908            0.002010  \n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight=\"balanced\", max_iter=2000),\n",
    "    \"RandomForest\": RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1),\n",
    "    \"XGB\": xgb.XGBClassifier(\n",
    "        eval_metric=\"aucpr\",\n",
    "        scale_pos_weight=len(y[y == 0]) / len(y[y == 1]),\n",
    "        n_jobs=-1,\n",
    "        verbosity=0,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Define smaller param grids\n",
    "param_grids = {\n",
    "    \"LogisticRegression\": [\n",
    "        {  # L2 penalty (most stable for small datasets)\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"solver\": [\"saga\"],\n",
    "            \"C\": np.logspace(-2, 1, 5),\n",
    "        },\n",
    "        {  # ElasticNet (optional)\n",
    "            \"penalty\": [\"elasticnet\"],\n",
    "            \"solver\": [\"saga\"],\n",
    "            \"C\": np.logspace(-2, 1, 5),\n",
    "            \"l1_ratio\": [0.2, 0.5, 0.8],\n",
    "        },\n",
    "    ],\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [5, 10, None],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\"],\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "        \"gamma\": [0, 1],\n",
    "        \"reg_alpha\": [0, 1],\n",
    "        \"reg_lambda\": [1, 5],\n",
    "    },\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "all_avg = []\n",
    "for name, model in models.items():\n",
    "    scale_needed = name in [\"LogisticRegression\", \"KNN\", \"SVC\"]\n",
    "    print(f\"\\n--- Running {name} ---\")\n",
    "    df_result, df_avg = train_evaluate_model_split(\n",
    "        model, param_grids[name], X, y, scale_data=scale_needed\n",
    "    )\n",
    "\n",
    "    # Tag model name explicitly\n",
    "    df_result[\"model\"] = name\n",
    "    df_avg[\"model\"] = name\n",
    "    all_results.append(df_result)\n",
    "    all_avg.append(df_avg)\n",
    "\n",
    "# Combine all results\n",
    "df_results_combined = pd.concat(all_results, ignore_index=True)\n",
    "df_avg_combined = pd.concat(all_avg, ignore_index=True)\n",
    "\n",
    "# Display\n",
    "print(\"\\nResults per model:\")\n",
    "print(\n",
    "    df_results_combined[\n",
    "        [\"model\", \"accuracy\", \"f1_score\", \"best_params\", \"confusion_matrix\"]\n",
    "    ]\n",
    ")\n",
    "print(\"\\nAverage metrics per model:\")\n",
    "print(df_avg_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
