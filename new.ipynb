{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c206d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. days: 185\n",
      "Date range: 2021-07-01 00:00:00 to 2022-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "# 1-2 layers hidden layers of 8-32 neurons\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# CONSTANTS\n",
    "TARGET = \"cases_new_increase_tmr\"\n",
    "START_DATE = pd.to_datetime(\"2021-07-01\")\n",
    "END_DATE = START_DATE + pd.DateOffset(months=6)\n",
    "NO_DAYS = (END_DATE - START_DATE).days + 1\n",
    "dates = pd.date_range(start=START_DATE, end=END_DATE, freq='D')\n",
    "print(f\"No. days: {NO_DAYS}\")\n",
    "print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "\n",
    "# Base GitHub URL\n",
    "BASE_URL = \"https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/\"\n",
    "\n",
    "# Relevant CSVs\n",
    "files = {\n",
    "    \"cases_malaysia\": BASE_URL + \"epidemic/cases_malaysia.csv\",\n",
    "    \"tests_malaysia\": BASE_URL + \"epidemic/tests_malaysia.csv\",\n",
    "    \"checkin_malaysia\": BASE_URL + \"mysejahtera/checkin_malaysia.csv\",\n",
    "    # \"deaths_malaysia\": BASE_URL + \"epidemic/deaths_malaysia.csv\",\n",
    "    # \"hospital\": BASE_URL + \"epidemic/hospital.csv\",\n",
    "    # \"icu\": BASE_URL + \"epidemic/icu.csv\",\n",
    "    # \"vax_malaysia\": BASE_URL + \"vaccination/vax_malaysia.csv\",\n",
    "    # \"trace_malaysia\": BASE_URL + \"mysejahtera/trace_malaysia.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1130495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases_malaysia: (185, 24)\n",
      "tests_malaysia: (185, 3)\n",
      "checkin_malaysia: (185, 4)\n"
     ]
    }
   ],
   "source": [
    "# Extract and laod all CSVs into DataFrames\n",
    "dfs = {name: pd.read_csv(url) for name, url in files.items()}\n",
    "\n",
    "# Clean data\n",
    "for k, df in dfs.items():\n",
    "    # Set date column as data\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df = df[(df[\"date\"] >= START_DATE) & (df[\"date\"] <= END_DATE)]\n",
    "\n",
    "    # Drop columns where ALL values are null\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Clean data\n",
    "    if k == \"trace_malaysia\":\n",
    "        # Handle duplicates\n",
    "        df = df.groupby(\"date\", as_index=False).mean()\n",
    "\n",
    "        # Interpolate data for missing dates\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df.set_index(\"date\", inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        df = df.reindex(dates)\n",
    "        cols = [\"casual_contacts\", \"hide_large\", \"hide_small\"]\n",
    "        df[cols] = df[cols].interpolate(method=\"linear\")\n",
    "        df = df.reset_index().rename(columns={\"index\": \"date\"})\n",
    "\n",
    "        # Fix columns type\n",
    "        df[cols] = df[cols].round().astype(int)\n",
    "\n",
    "    # Remove columns with one value only\n",
    "    df = df.loc[:, df.nunique(dropna=True) > 1]\n",
    "\n",
    "    # Save back into dictionary\n",
    "    dfs[k] = df\n",
    "\n",
    "    # Check shape\n",
    "    output_filename = f\"{k}.csv\"\n",
    "    print(f\"{k}: {df.shape}\")\n",
    "\n",
    "# Merge data\n",
    "data = pd.DataFrame({'date': dates})\n",
    "for k, df in dfs.items():\n",
    "    if k == \"population\":\n",
    "        continue\n",
    "    if df.shape[0] != NO_DAYS:\n",
    "        df = df.drop(columns=[\"state\"])\n",
    "        df = df.groupby(\"date\").sum()\n",
    "    data = pd.merge(data, df, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40e98f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (178, 21)\n",
      "y shape: (178,)\n",
      "Selected features: ['cases_new', 'cases_new_shift1', 'cases_new_7d_avg', 'cases_new_pct_change', 'cases_active', 'cases_active_shift1', 'cases_active_7d_avg', 'cases_active_pct_change', 'cases_cluster', 'cases_cluster_shift1', 'cases_cluster_7d_avg', 'cases_cluster_pct_change', 'tests_total', 'tests_total_shift1', 'tests_total_7d_avg', 'tests_total_pct_change', 'mobility_density', 'mobility_density_shift1', 'mobility_density_7d_avg', 'mobility_density_pct_change', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "# Include target cases_new_increase_tmr\n",
    "data['cases_new_increase_tmr'] = (data['cases_new'].shift(-1) > data['cases_new']).astype(int)\n",
    "\n",
    "# Drop last row (no target)\n",
    "data = data[:-1]\n",
    "\n",
    "\n",
    "# Combine features\n",
    "data['tests_total'] = data['rtk-ag'] + data['pcr']  # combine tests\n",
    "data['mobility_density'] = data['checkins'] / data['unique_loc']  # density of people per location\n",
    "data['mobility_density'] = data['mobility_density'].replace([float('inf'), -float('inf')], 0).fillna(0)\n",
    "\n",
    "# Lag features\n",
    "lag_cols = ['cases_new', 'cases_active', 'cases_cluster', 'tests_total', 'mobility_density']\n",
    "for col in lag_cols:\n",
    "    data[f'{col}_shift1'] = data[col].shift(1)  # previous day\n",
    "\n",
    "# Rolling averages (7-day)\n",
    "for col in lag_cols:\n",
    "    data[f'{col}_7d_avg'] = data[col].rolling(window=7).mean()  # 7-day avg\n",
    "\n",
    "# Percent change\n",
    "for col in lag_cols:\n",
    "    data[f'{col}_pct_change'] = data[col].pct_change()  # daily pct change\n",
    "\n",
    "# Day of week\n",
    "data['day_of_week'] = data['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "\n",
    "# Drop rows with NaN caused by lag/rolling\n",
    "data = data.dropna().reset_index(drop=True)  # clean data\n",
    "\n",
    "# Target\n",
    "y = data['cases_new_increase_tmr']  # target\n",
    "\n",
    "# Features selection\n",
    "feature_cols = [\n",
    "    'cases_new', 'cases_new_shift1', 'cases_new_7d_avg', 'cases_new_pct_change',\n",
    "    'cases_active', 'cases_active_shift1', 'cases_active_7d_avg', 'cases_active_pct_change',\n",
    "    'cases_cluster', 'cases_cluster_shift1', 'cases_cluster_7d_avg', 'cases_cluster_pct_change',\n",
    "    'tests_total', 'tests_total_shift1', 'tests_total_7d_avg', 'tests_total_pct_change',\n",
    "    'mobility_density', 'mobility_density_shift1', 'mobility_density_7d_avg', 'mobility_density_pct_change',\n",
    "    'day_of_week'\n",
    "]\n",
    "\n",
    "X = data[feature_cols]  # features\n",
    "\n",
    "# Output shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Selected features:\", feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def train_evaluate_model_df(model, param_grid, X, y, scale_data=False, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model using TimeSeriesSplit + GridSearchCV, returns results in DataFrame.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    rows = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Scaling if needed\n",
    "        if scale_data:\n",
    "            scaler = RobustScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_val = scaler.transform(X_val)\n",
    "        else:\n",
    "            X_train = X_train.values\n",
    "            X_val = X_val.values\n",
    "\n",
    "        # GridSearchCV for hyperparameter tuning\n",
    "        grid = GridSearchCV(model, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_val)\n",
    "\n",
    "        # Store fold metrics\n",
    "        rows.append({\n",
    "            'model': type(model).__name__,\n",
    "            'fold': fold,\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'f1_score': f1_score(y_val, y_pred),\n",
    "            'best_params': grid.best_params_,\n",
    "            'confusion_matrix': confusion_matrix(y_val, y_pred)\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(rows)\n",
    "    df_avg = pd.DataFrame({\n",
    "        'model': [type(model).__name__],\n",
    "        'average_accuracy': [df_results['accuracy'].mean()],\n",
    "        'average_f1': [df_results['f1_score'].mean()]\n",
    "    })\n",
    "\n",
    "    return df_results, df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa1a4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVC\": SVC(),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGB\": xgb.XGBClassifier(\n",
    "        use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "        \"max_iter\": [1000],\n",
    "    },\n",
    "    \"KNN\": {\"n_neighbors\": [3, 5, 7], \"weights\": [\"uniform\", \"distance\"], \"p\": [1, 2]},\n",
    "    \"SVC\": {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"], \"gamma\": [\"scale\", \"auto\"]},\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [50, 100],\n",
    "        \"max_depth\": [None, 5, 10],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"n_estimators\": [50, 100],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53712dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [15:01:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [15:01:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [15:01:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [15:01:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-wise results:\n",
      "                     model  fold  accuracy  f1_score  \\\n",
      "0       LogisticRegression     1  0.793103  0.812500   \n",
      "1       LogisticRegression     2  0.655172  0.666667   \n",
      "2       LogisticRegression     3  0.862069  0.846154   \n",
      "3       LogisticRegression     4  0.827586  0.838710   \n",
      "4       LogisticRegression     5  0.862069  0.857143   \n",
      "5     KNeighborsClassifier     1  0.655172  0.642857   \n",
      "6     KNeighborsClassifier     2  0.724138  0.555556   \n",
      "7     KNeighborsClassifier     3  0.896552  0.880000   \n",
      "8     KNeighborsClassifier     4  0.827586  0.838710   \n",
      "9     KNeighborsClassifier     5  0.896552  0.888889   \n",
      "10                     SVC     1  0.482759  0.651163   \n",
      "11                     SVC     2  0.344828  0.512821   \n",
      "12                     SVC     3  0.758621  0.666667   \n",
      "13                     SVC     4  0.689655  0.709677   \n",
      "14                     SVC     5  0.896552  0.888889   \n",
      "15  RandomForestClassifier     1  0.689655  0.666667   \n",
      "16  RandomForestClassifier     2  0.551724  0.606061   \n",
      "17  RandomForestClassifier     3  0.827586  0.814815   \n",
      "18  RandomForestClassifier     4  0.896552  0.903226   \n",
      "19  RandomForestClassifier     5  0.827586  0.838710   \n",
      "20           XGBClassifier     1  0.724138  0.750000   \n",
      "21           XGBClassifier     2  0.482759  0.516129   \n",
      "22           XGBClassifier     3  0.724138  0.600000   \n",
      "23           XGBClassifier     4  0.931034  0.928571   \n",
      "24           XGBClassifier     5  0.862069  0.846154   \n",
      "\n",
      "                                          best_params    confusion_matrix  \n",
      "0   {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2',...  [[10, 5], [1, 13]]  \n",
      "1   {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2',...  [[9, 10], [0, 10]]  \n",
      "2   {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', ...  [[14, 1], [3, 11]]  \n",
      "3   {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2',...  [[11, 3], [2, 13]]  \n",
      "4   {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', ...  [[13, 3], [1, 12]]  \n",
      "5    {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}   [[10, 5], [5, 9]]  \n",
      "6    {'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}   [[16, 3], [5, 5]]  \n",
      "7    {'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}  [[15, 0], [3, 11]]  \n",
      "8    {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}  [[11, 3], [2, 13]]  \n",
      "9    {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}  [[14, 2], [1, 12]]  \n",
      "10      {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}  [[0, 15], [0, 14]]  \n",
      "11      {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}  [[0, 19], [0, 10]]  \n",
      "12     {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}   [[15, 0], [7, 7]]  \n",
      "13    {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}   [[9, 5], [4, 11]]  \n",
      "14     {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}  [[14, 2], [1, 12]]  \n",
      "15  {'max_depth': None, 'min_samples_split': 2, 'n...   [[11, 4], [5, 9]]  \n",
      "16  {'max_depth': None, 'min_samples_split': 2, 'n...  [[6, 13], [0, 10]]  \n",
      "17  {'max_depth': 5, 'min_samples_split': 5, 'n_es...  [[13, 2], [3, 11]]  \n",
      "18  {'max_depth': None, 'min_samples_split': 2, 'n...  [[12, 2], [1, 14]]  \n",
      "19  {'max_depth': None, 'min_samples_split': 5, 'n...  [[11, 5], [0, 13]]  \n",
      "20  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...   [[9, 6], [2, 12]]  \n",
      "21  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...   [[6, 13], [2, 8]]  \n",
      "22  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...   [[15, 0], [8, 6]]  \n",
      "23  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  [[14, 0], [2, 13]]  \n",
      "24  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  [[14, 2], [2, 11]]  \n",
      "\n",
      "Average metrics per model:\n",
      "                    model  average_accuracy  average_f1\n",
      "0      LogisticRegression          0.800000    0.804235\n",
      "1    KNeighborsClassifier          0.800000    0.761202\n",
      "2                     SVC          0.634483    0.685843\n",
      "3  RandomForestClassifier          0.758621    0.765896\n",
      "4           XGBClassifier          0.744828    0.728171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [15:01:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "all_folds = []\n",
    "all_avg = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scale_needed = name in ['LogisticRegression','KNN','SVC']\n",
    "    df_folds, df_avg = train_evaluate_model_df(model, param_grids[name], X, y, scale_data=scale_needed)\n",
    "    all_folds.append(df_folds)\n",
    "    all_avg.append(df_avg)\n",
    "\n",
    "# Combine all results\n",
    "df_folds_combined = pd.concat(all_folds, ignore_index=True)\n",
    "df_avg_combined = pd.concat(all_avg, ignore_index=True)\n",
    "\n",
    "# Display\n",
    "print(\"Fold-wise results:\")\n",
    "print(df_folds_combined)\n",
    "print(\"\\nAverage metrics per model:\")\n",
    "print(df_avg_combined)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
